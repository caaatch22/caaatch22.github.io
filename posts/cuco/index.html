<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>cuCollections | Mingjie&#39;s Home</title>
<meta name="keywords" content="[CUDA]">
<meta name="description" content="
still draft, to be updated

实现了一个dynamic_map的retrieve_all

访存算子，经典过滤问题，predict为true的元素拷贝到out数组，重点是需要维护一个 atomic 的out index, https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/, 翻译：https://zhuanlan.zhihu.com/p/581078557

host端：
template &lt;typename Key, typename Value, cuda::thread_scope Scope, typename Allocator&gt;
template &lt;typename KeyOut, typename ValueOut&gt;
std::pair&lt;KeyOut, ValueOut&gt; dynamic_map&lt;Key, Value, Scope, Allocator&gt;::retrieve_all(
  KeyOut keys_out, ValueOut values_out, cudaStream_t stream) const
{
  auto constexpr block_size = 128;
  auto constexpr stride     = 1;

  auto const capacity       = get_capacity();
  auto grid_size            = (capacity &#43; stride * block_size - 1) / (stride * block_size);

  std::vector&lt;size_t&gt; submap_cap_prefix(submaps_.size());
  std::inclusive_scan(submaps_.begin(), submaps_.end(), submap_cap_prefix.begin(),
                      [](auto const&amp; sum, auto const&amp; submap) { return sum &#43; submap-&gt;get_capacity(); }, 
                      (size_t)0);
  thrust::device_vector&lt;size_t&gt; submap_cap_prefix_d(submap_cap_prefix);

  // 复用alloc_（用于slots_的alloc_）会比直接cudaMalloc快一个数量级，不需要重新分配内存
  // 单纯cudaMalloc会触发GPU driver/runtime 的 allocation 初始化、页表建立等
  using temp_allocator_type =
    typename std::allocator_traits&lt;Allocator&gt;::template rebind_alloc&lt;char&gt;;
  auto temp_allocator = temp_allocator_type{alloc_};
  auto d_num_out      = reinterpret_cast&lt;unsigned long long*&gt;(
    std::allocator_traits&lt;temp_allocator_type&gt;::allocate(temp_allocator, sizeof(unsigned long long)));
  CUCO_CUDA_TRY(cudaMemsetAsync(d_num_out, 0, sizeof(unsigned long long), stream));
  
  detail::retrieve_all&lt;block_size&gt;&lt;&lt;&lt;grid_size, block_size, 0, stream&gt;&gt;&gt;(
    keys_out, values_out, submap_views_.data().get(), submaps_.size(), 
    capacity, d_num_out, submap_cap_prefix_d.data().get(), 
    empty_key_sentinel_, erased_key_sentinel_);

  size_t h_num_out;
  CUCO_CUDA_TRY(
    cudaMemcpyAsync(&amp;h_num_out, d_num_out, sizeof(size_t), cudaMemcpyDeviceToHost, stream));
  CUCO_CUDA_TRY(cudaStreamSynchronize(stream));
  CUCO_CUDA_TRY(cudaFree(d_num_out))
  return {keys_out &#43; h_num_out, values_out &#43; h_num_out};
}

naive实现，一个全局atomic

template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    // prefix_sum长度一般就10以内，不需要二分之类的操作
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    if (is_filled) {
      auto idx   = atomicAdd(d_num_out, static_cast&lt;unsigned long long&gt;(1));
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[idx]    = existing_key;
      values_out[idx]  = value;
    }
  }
}

block内atomicAdd &#43; 全局atomicAdd

// 一个block内用一个__shared__ local_count表示这个block中predict为true的数量
// local_pos表示当前线程在该block内第几个predict为true

template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  __shared__ unsigned int local_count;
  if (threadIdx.x == 0) {
    local_count = 0;
  }
  __syncthreads();

  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    unsigned int local_pos = 0;
    if (is_filled) {
      local_pos = atomicAdd_block(&amp;local_count, 1);
    }
    __syncthreads();

    if (threadIdx.x == 0) {
      local_count = atomicAdd(d_num_out, local_count);
    }
    __syncthreads();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[local_count &#43; local_pos]    = existing_key;
      values_out[local_count &#43; local_pos]  = value;
    }
  }
}


// 类似原理，但是用cub::BlockScan实现
template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  using BlockScan = cub::BlockScan&lt;unsigned int, block_size&gt;;

  // Shared memory
  __shared__ typename BlockScan::TempStorage scan_temp_storage;
  __shared__ unsigned int block_base;

  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  for (; tid &lt; capacity; tid &#43;= stride) {
    // Compute submap index and offset
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps) &#43;&#43;submap_idx;
    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }

    auto const&amp; current_slot = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key   = current_slot.first.load(cuda::std::memory_order_relaxed);

    // Check key validity
    bool is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) ||
                         cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    // Perform block-wide exclusive scan to compute local write index
    unsigned int local_idx = 0;
    unsigned int total_valid = 0;
    BlockScan(scan_temp_storage).ExclusiveSum(is_filled ? 1u : 0u, local_idx, total_valid);

    // Block leader calculates global offset
    if (threadIdx.x == 0) {
      block_base = atomicAdd(d_num_out, total_valid);
    }
    __syncthreads();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[block_base &#43; local_idx]   = existing_key;
      values_out[block_base &#43; local_idx] = value;
    }
  }
}

3. warp-aggregated atomics: warp(or cooperative group)粒度atomicAdd &#43; block内atomicAdd&#43;全局atomicAdd
```cpp
template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  __shared__ unsigned int block_count;
  __shared__ unsigned int block_base;
  if (threadIdx.x == 0) {
    block_count = 0;
    block_base = 0;
  }
  __syncthreads();

  unsigned int local_idx = 0;
  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    unsigned mask = __ballot_sync(0xffffffff, is_filled);
    int lane = threadIdx.x &amp; 0x1f;
    int warp_prefix = __popc(mask &amp; ((1u &lt;&lt; lane) - 1));
    if (is_filled) local_idx = warp_prefix;

    unsigned int warp_vote = __popc(mask);
    unsigned int warp_base = 0;
    if (lane == 0 &amp;&amp; warp_vote) {
      warp_base = atomicAdd_block(&amp;block_count, warp_vote);
    }
    warp_base = __shfl_sync(0xffffffff, warp_base, 0);
    __syncthreads();

    if (threadIdx.x == 0) {
      block_base = atomicAdd(d_num_out, block_count);
    }
    __syncthreads();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[block_base &#43; warp_base &#43; local_idx]    = existing_key;
      values_out[block_base &#43; warp_base &#43; local_idx]  = value;
    }
  }
}

// 类似的，但是用cooperative group实现,实测还是tile_size=32最快，和warp没区别
// 写起来更modern一点
template &lt;uint32_t block_size,
          uint32_t tile_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tile   = cg::tiled_partition&lt;tile_size&gt;(cg::this_thread_block());
  auto block  = cg::this_thread_block();
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  __shared__ unsigned int block_count;
  __shared__ unsigned int block_base;
  if (threadIdx.x == 0) {
    block_count = 0;
    block_base = 0;
  }
  block.sync();

  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    unsigned int tile_mask = tile.ballot(is_filled);
    unsigned int tile_rank = tile.thread_rank();
    unsigned int tile_vote = __popc(tile_mask);
    unsigned int tile_prefix = __popc(tile_mask &amp; ((1u &lt;&lt; tile_rank) - 1));
    
    unsigned int tile_base = 0;
    if (tile_rank == 0 &amp;&amp; tile_mask) {
      tile_base = atomicAdd_block(&amp;block_count, tile_vote);
    }
    tile_base = tile.shfl(tile_base, 0);
    block.sync();

    if (block.thread_rank() == 0) {
      block_base = atomicAdd(d_num_out, block_count);
    }
    block.sync();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[block_base &#43; tile_base &#43; tile_prefix]    = existing_key;
      values_out[block_base &#43; tile_base &#43; tile_prefix]  = value;
    }
  }
}
实测2/3速度差不多，在插入1亿数据后（实际总cap达到2亿），&lt;key, value&gt;都是cuda::atomic&lt;int64_t&gt;的情况下，retrieve_all cost 3ms左右；">
<meta name="author" content="">
<link rel="canonical" href="https://caaatch22.github.io/posts/cuco/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.678b5c47efa744d2e0dd0d61101075e6aecdc9a0631e7ad8538f4ec0cca79273.css" integrity="sha256-Z4tcR&#43;&#43;nRNLg3Q1hEBB15q7NyaBjHnrYU49OwMynknM=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://caaatch22.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://caaatch22.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://caaatch22.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://caaatch22.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://caaatch22.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://caaatch22.github.io/posts/cuco/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WEG841BBW9"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-WEG841BBW9');
        }
      </script><meta property="og:title" content="cuCollections" />
<meta property="og:description"
  content="
still draft, to be updated

实现了一个dynamic_map的retrieve_all

访存算子，经典过滤问题，predict为true的元素拷贝到out数组，重点是需要维护一个 atomic 的out index, https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/, 翻译：https://zhuanlan.zhihu.com/p/581078557

host端：
template &lt;typename Key, typename Value, cuda::thread_scope Scope, typename Allocator&gt;
template &lt;typename KeyOut, typename ValueOut&gt;
std::pair&lt;KeyOut, ValueOut&gt; dynamic_map&lt;Key, Value, Scope, Allocator&gt;::retrieve_all(
  KeyOut keys_out, ValueOut values_out, cudaStream_t stream) const
{
  auto constexpr block_size = 128;
  auto constexpr stride     = 1;

  auto const capacity       = get_capacity();
  auto grid_size            = (capacity &#43; stride * block_size - 1) / (stride * block_size);

  std::vector&lt;size_t&gt; submap_cap_prefix(submaps_.size());
  std::inclusive_scan(submaps_.begin(), submaps_.end(), submap_cap_prefix.begin(),
                      [](auto const&amp; sum, auto const&amp; submap) { return sum &#43; submap-&gt;get_capacity(); }, 
                      (size_t)0);
  thrust::device_vector&lt;size_t&gt; submap_cap_prefix_d(submap_cap_prefix);

  // 复用alloc_（用于slots_的alloc_）会比直接cudaMalloc快一个数量级，不需要重新分配内存
  // 单纯cudaMalloc会触发GPU driver/runtime 的 allocation 初始化、页表建立等
  using temp_allocator_type =
    typename std::allocator_traits&lt;Allocator&gt;::template rebind_alloc&lt;char&gt;;
  auto temp_allocator = temp_allocator_type{alloc_};
  auto d_num_out      = reinterpret_cast&lt;unsigned long long*&gt;(
    std::allocator_traits&lt;temp_allocator_type&gt;::allocate(temp_allocator, sizeof(unsigned long long)));
  CUCO_CUDA_TRY(cudaMemsetAsync(d_num_out, 0, sizeof(unsigned long long), stream));
  
  detail::retrieve_all&lt;block_size&gt;&lt;&lt;&lt;grid_size, block_size, 0, stream&gt;&gt;&gt;(
    keys_out, values_out, submap_views_.data().get(), submaps_.size(), 
    capacity, d_num_out, submap_cap_prefix_d.data().get(), 
    empty_key_sentinel_, erased_key_sentinel_);

  size_t h_num_out;
  CUCO_CUDA_TRY(
    cudaMemcpyAsync(&amp;h_num_out, d_num_out, sizeof(size_t), cudaMemcpyDeviceToHost, stream));
  CUCO_CUDA_TRY(cudaStreamSynchronize(stream));
  CUCO_CUDA_TRY(cudaFree(d_num_out))
  return {keys_out &#43; h_num_out, values_out &#43; h_num_out};
}

naive实现，一个全局atomic

template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    // prefix_sum长度一般就10以内，不需要二分之类的操作
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    if (is_filled) {
      auto idx   = atomicAdd(d_num_out, static_cast&lt;unsigned long long&gt;(1));
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[idx]    = existing_key;
      values_out[idx]  = value;
    }
  }
}

block内atomicAdd &#43; 全局atomicAdd

// 一个block内用一个__shared__ local_count表示这个block中predict为true的数量
// local_pos表示当前线程在该block内第几个predict为true

template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  __shared__ unsigned int local_count;
  if (threadIdx.x == 0) {
    local_count = 0;
  }
  __syncthreads();

  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    unsigned int local_pos = 0;
    if (is_filled) {
      local_pos = atomicAdd_block(&amp;local_count, 1);
    }
    __syncthreads();

    if (threadIdx.x == 0) {
      local_count = atomicAdd(d_num_out, local_count);
    }
    __syncthreads();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[local_count &#43; local_pos]    = existing_key;
      values_out[local_count &#43; local_pos]  = value;
    }
  }
}


// 类似原理，但是用cub::BlockScan实现
template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  using BlockScan = cub::BlockScan&lt;unsigned int, block_size&gt;;

  // Shared memory
  __shared__ typename BlockScan::TempStorage scan_temp_storage;
  __shared__ unsigned int block_base;

  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  for (; tid &lt; capacity; tid &#43;= stride) {
    // Compute submap index and offset
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps) &#43;&#43;submap_idx;
    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }

    auto const&amp; current_slot = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key   = current_slot.first.load(cuda::std::memory_order_relaxed);

    // Check key validity
    bool is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) ||
                         cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    // Perform block-wide exclusive scan to compute local write index
    unsigned int local_idx = 0;
    unsigned int total_valid = 0;
    BlockScan(scan_temp_storage).ExclusiveSum(is_filled ? 1u : 0u, local_idx, total_valid);

    // Block leader calculates global offset
    if (threadIdx.x == 0) {
      block_base = atomicAdd(d_num_out, total_valid);
    }
    __syncthreads();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[block_base &#43; local_idx]   = existing_key;
      values_out[block_base &#43; local_idx] = value;
    }
  }
}

3. warp-aggregated atomics: warp(or cooperative group)粒度atomicAdd &#43; block内atomicAdd&#43;全局atomicAdd
```cpp
template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  __shared__ unsigned int block_count;
  __shared__ unsigned int block_base;
  if (threadIdx.x == 0) {
    block_count = 0;
    block_base = 0;
  }
  __syncthreads();

  unsigned int local_idx = 0;
  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    unsigned mask = __ballot_sync(0xffffffff, is_filled);
    int lane = threadIdx.x &amp; 0x1f;
    int warp_prefix = __popc(mask &amp; ((1u &lt;&lt; lane) - 1));
    if (is_filled) local_idx = warp_prefix;

    unsigned int warp_vote = __popc(mask);
    unsigned int warp_base = 0;
    if (lane == 0 &amp;&amp; warp_vote) {
      warp_base = atomicAdd_block(&amp;block_count, warp_vote);
    }
    warp_base = __shfl_sync(0xffffffff, warp_base, 0);
    __syncthreads();

    if (threadIdx.x == 0) {
      block_base = atomicAdd(d_num_out, block_count);
    }
    __syncthreads();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[block_base &#43; warp_base &#43; local_idx]    = existing_key;
      values_out[block_base &#43; warp_base &#43; local_idx]  = value;
    }
  }
}

// 类似的，但是用cooperative group实现,实测还是tile_size=32最快，和warp没区别
// 写起来更modern一点
template &lt;uint32_t block_size,
          uint32_t tile_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tile   = cg::tiled_partition&lt;tile_size&gt;(cg::this_thread_block());
  auto block  = cg::this_thread_block();
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  __shared__ unsigned int block_count;
  __shared__ unsigned int block_base;
  if (threadIdx.x == 0) {
    block_count = 0;
    block_base = 0;
  }
  block.sync();

  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    unsigned int tile_mask = tile.ballot(is_filled);
    unsigned int tile_rank = tile.thread_rank();
    unsigned int tile_vote = __popc(tile_mask);
    unsigned int tile_prefix = __popc(tile_mask &amp; ((1u &lt;&lt; tile_rank) - 1));
    
    unsigned int tile_base = 0;
    if (tile_rank == 0 &amp;&amp; tile_mask) {
      tile_base = atomicAdd_block(&amp;block_count, tile_vote);
    }
    tile_base = tile.shfl(tile_base, 0);
    block.sync();

    if (block.thread_rank() == 0) {
      block_base = atomicAdd(d_num_out, block_count);
    }
    block.sync();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[block_base &#43; tile_base &#43; tile_prefix]    = existing_key;
      values_out[block_base &#43; tile_base &#43; tile_prefix]  = value;
    }
  }
}
实测2/3速度差不多，在插入1亿数据后（实际总cap达到2亿），&lt;key, value&gt;都是cuda::atomic&lt;int64_t&gt;的情况下，retrieve_all cost 3ms左右；" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://caaatch22.github.io/posts/cuco/" /><meta property="article:section" content="posts" />

<meta property="article:published_time" content="2025-04-28T00:00:00+00:00" />

<meta property="article:modified_time" content="2025-04-28T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="cuCollections" />
<meta name="twitter:description"
    content="
still draft, to be updated

实现了一个dynamic_map的retrieve_all

访存算子，经典过滤问题，predict为true的元素拷贝到out数组，重点是需要维护一个 atomic 的out index, https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/, 翻译：https://zhuanlan.zhihu.com/p/581078557

host端：
template &lt;typename Key, typename Value, cuda::thread_scope Scope, typename Allocator&gt;
template &lt;typename KeyOut, typename ValueOut&gt;
std::pair&lt;KeyOut, ValueOut&gt; dynamic_map&lt;Key, Value, Scope, Allocator&gt;::retrieve_all(
  KeyOut keys_out, ValueOut values_out, cudaStream_t stream) const
{
  auto constexpr block_size = 128;
  auto constexpr stride     = 1;

  auto const capacity       = get_capacity();
  auto grid_size            = (capacity &#43; stride * block_size - 1) / (stride * block_size);

  std::vector&lt;size_t&gt; submap_cap_prefix(submaps_.size());
  std::inclusive_scan(submaps_.begin(), submaps_.end(), submap_cap_prefix.begin(),
                      [](auto const&amp; sum, auto const&amp; submap) { return sum &#43; submap-&gt;get_capacity(); }, 
                      (size_t)0);
  thrust::device_vector&lt;size_t&gt; submap_cap_prefix_d(submap_cap_prefix);

  // 复用alloc_（用于slots_的alloc_）会比直接cudaMalloc快一个数量级，不需要重新分配内存
  // 单纯cudaMalloc会触发GPU driver/runtime 的 allocation 初始化、页表建立等
  using temp_allocator_type =
    typename std::allocator_traits&lt;Allocator&gt;::template rebind_alloc&lt;char&gt;;
  auto temp_allocator = temp_allocator_type{alloc_};
  auto d_num_out      = reinterpret_cast&lt;unsigned long long*&gt;(
    std::allocator_traits&lt;temp_allocator_type&gt;::allocate(temp_allocator, sizeof(unsigned long long)));
  CUCO_CUDA_TRY(cudaMemsetAsync(d_num_out, 0, sizeof(unsigned long long), stream));
  
  detail::retrieve_all&lt;block_size&gt;&lt;&lt;&lt;grid_size, block_size, 0, stream&gt;&gt;&gt;(
    keys_out, values_out, submap_views_.data().get(), submaps_.size(), 
    capacity, d_num_out, submap_cap_prefix_d.data().get(), 
    empty_key_sentinel_, erased_key_sentinel_);

  size_t h_num_out;
  CUCO_CUDA_TRY(
    cudaMemcpyAsync(&amp;h_num_out, d_num_out, sizeof(size_t), cudaMemcpyDeviceToHost, stream));
  CUCO_CUDA_TRY(cudaStreamSynchronize(stream));
  CUCO_CUDA_TRY(cudaFree(d_num_out))
  return {keys_out &#43; h_num_out, values_out &#43; h_num_out};
}

naive实现，一个全局atomic

template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    // prefix_sum长度一般就10以内，不需要二分之类的操作
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    if (is_filled) {
      auto idx   = atomicAdd(d_num_out, static_cast&lt;unsigned long long&gt;(1));
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[idx]    = existing_key;
      values_out[idx]  = value;
    }
  }
}

block内atomicAdd &#43; 全局atomicAdd

// 一个block内用一个__shared__ local_count表示这个block中predict为true的数量
// local_pos表示当前线程在该block内第几个predict为true

template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  __shared__ unsigned int local_count;
  if (threadIdx.x == 0) {
    local_count = 0;
  }
  __syncthreads();

  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    unsigned int local_pos = 0;
    if (is_filled) {
      local_pos = atomicAdd_block(&amp;local_count, 1);
    }
    __syncthreads();

    if (threadIdx.x == 0) {
      local_count = atomicAdd(d_num_out, local_count);
    }
    __syncthreads();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[local_count &#43; local_pos]    = existing_key;
      values_out[local_count &#43; local_pos]  = value;
    }
  }
}


// 类似原理，但是用cub::BlockScan实现
template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  using BlockScan = cub::BlockScan&lt;unsigned int, block_size&gt;;

  // Shared memory
  __shared__ typename BlockScan::TempStorage scan_temp_storage;
  __shared__ unsigned int block_base;

  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  for (; tid &lt; capacity; tid &#43;= stride) {
    // Compute submap index and offset
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps) &#43;&#43;submap_idx;
    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }

    auto const&amp; current_slot = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key   = current_slot.first.load(cuda::std::memory_order_relaxed);

    // Check key validity
    bool is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) ||
                         cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    // Perform block-wide exclusive scan to compute local write index
    unsigned int local_idx = 0;
    unsigned int total_valid = 0;
    BlockScan(scan_temp_storage).ExclusiveSum(is_filled ? 1u : 0u, local_idx, total_valid);

    // Block leader calculates global offset
    if (threadIdx.x == 0) {
      block_base = atomicAdd(d_num_out, total_valid);
    }
    __syncthreads();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[block_base &#43; local_idx]   = existing_key;
      values_out[block_base &#43; local_idx] = value;
    }
  }
}

3. warp-aggregated atomics: warp(or cooperative group)粒度atomicAdd &#43; block内atomicAdd&#43;全局atomicAdd
```cpp
template &lt;uint32_t block_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  __shared__ unsigned int block_count;
  __shared__ unsigned int block_base;
  if (threadIdx.x == 0) {
    block_count = 0;
    block_base = 0;
  }
  __syncthreads();

  unsigned int local_idx = 0;
  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    unsigned mask = __ballot_sync(0xffffffff, is_filled);
    int lane = threadIdx.x &amp; 0x1f;
    int warp_prefix = __popc(mask &amp; ((1u &lt;&lt; lane) - 1));
    if (is_filled) local_idx = warp_prefix;

    unsigned int warp_vote = __popc(mask);
    unsigned int warp_base = 0;
    if (lane == 0 &amp;&amp; warp_vote) {
      warp_base = atomicAdd_block(&amp;block_count, warp_vote);
    }
    warp_base = __shfl_sync(0xffffffff, warp_base, 0);
    __syncthreads();

    if (threadIdx.x == 0) {
      block_base = atomicAdd(d_num_out, block_count);
    }
    __syncthreads();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[block_base &#43; warp_base &#43; local_idx]    = existing_key;
      values_out[block_base &#43; warp_base &#43; local_idx]  = value;
    }
  }
}

// 类似的，但是用cooperative group实现,实测还是tile_size=32最快，和warp没区别
// 写起来更modern一点
template &lt;uint32_t block_size,
          uint32_t tile_size,
          typename OutputIt,
          typename viewT,
          typename PrefixT,
          typename Key&gt;
CUCO_KERNEL void retrieve_all(OutputIt keys_out,
                              OutputIt values_out,
                              viewT* submap_views,
                              uint32_t num_submaps,
                              uint64_t capacity,
                              unsigned long long* d_num_out,
                              PrefixT* prefix_sum,
                              Key empty_key_sentinel,
                              Key erased_key_sentinel)
{
  auto tile   = cg::tiled_partition&lt;tile_size&gt;(cg::this_thread_block());
  auto block  = cg::this_thread_block();
  auto tid    = blockDim.x * blockIdx.x &#43; threadIdx.x;
  auto stride = blockDim.x * gridDim.x;

  __shared__ unsigned int block_count;
  __shared__ unsigned int block_base;
  if (threadIdx.x == 0) {
    block_count = 0;
    block_base = 0;
  }
  block.sync();

  for (; tid &lt; capacity; tid &#43;= stride) {
    uint32_t submap_idx = 0;
    uint32_t submap_offset = tid;
    while (tid &gt;= prefix_sum[submap_idx] &amp;&amp; submap_idx &lt; num_submaps ) &#43;&#43;submap_idx;

    if (submap_idx &gt; 0) {
      submap_offset = tid - prefix_sum[submap_idx - 1];
    }
    auto const &amp;current_slot  = submap_views[submap_idx].get_slots()[submap_offset];
    Key const existing_key    = current_slot.first.load(cuda::std::memory_order_relaxed);
    auto const is_filled =
      not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or
          cuco::detail::bitwise_compare(existing_key, erased_key_sentinel));

    unsigned int tile_mask = tile.ballot(is_filled);
    unsigned int tile_rank = tile.thread_rank();
    unsigned int tile_vote = __popc(tile_mask);
    unsigned int tile_prefix = __popc(tile_mask &amp; ((1u &lt;&lt; tile_rank) - 1));
    
    unsigned int tile_base = 0;
    if (tile_rank == 0 &amp;&amp; tile_mask) {
      tile_base = atomicAdd_block(&amp;block_count, tile_vote);
    }
    tile_base = tile.shfl(tile_base, 0);
    block.sync();

    if (block.thread_rank() == 0) {
      block_base = atomicAdd(d_num_out, block_count);
    }
    block.sync();

    if (is_filled) {
      auto value = current_slot.second.load(cuda::std::memory_order_relaxed);
      keys_out[block_base &#43; tile_base &#43; tile_prefix]    = existing_key;
      values_out[block_base &#43; tile_base &#43; tile_prefix]  = value;
    }
  }
}
实测2/3速度差不多，在插入1亿数据后（实际总cap达到2亿），&lt;key, value&gt;都是cuda::atomic&lt;int64_t&gt;的情况下，retrieve_all cost 3ms左右；" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://caaatch22.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "cuCollections",
      "item": "https://caaatch22.github.io/posts/cuco/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "cuCollections",
  "name": "cuCollections",
  "description": " still draft, to be updated\n实现了一个dynamic_map的retrieve_all 访存算子，经典过滤问题，predict为true的元素拷贝到out数组，重点是需要维护一个 atomic 的out index, https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/, 翻译：https://zhuanlan.zhihu.com/p/581078557\nhost端：\ntemplate \u0026lt;typename Key, typename Value, cuda::thread_scope Scope, typename Allocator\u0026gt; template \u0026lt;typename KeyOut, typename ValueOut\u0026gt; std::pair\u0026lt;KeyOut, ValueOut\u0026gt; dynamic_map\u0026lt;Key, Value, Scope, Allocator\u0026gt;::retrieve_all( KeyOut keys_out, ValueOut values_out, cudaStream_t stream) const { auto constexpr block_size = 128; auto constexpr stride = 1; auto const capacity = get_capacity(); auto grid_size = (capacity + stride * block_size - 1) / (stride * block_size); std::vector\u0026lt;size_t\u0026gt; submap_cap_prefix(submaps_.size()); std::inclusive_scan(submaps_.begin(), submaps_.end(), submap_cap_prefix.begin(), [](auto const\u0026amp; sum, auto const\u0026amp; submap) { return sum + submap-\u0026gt;get_capacity(); }, (size_t)0); thrust::device_vector\u0026lt;size_t\u0026gt; submap_cap_prefix_d(submap_cap_prefix); // 复用alloc_（用于slots_的alloc_）会比直接cudaMalloc快一个数量级，不需要重新分配内存 // 单纯cudaMalloc会触发GPU driver/runtime 的 allocation 初始化、页表建立等 using temp_allocator_type = typename std::allocator_traits\u0026lt;Allocator\u0026gt;::template rebind_alloc\u0026lt;char\u0026gt;; auto temp_allocator = temp_allocator_type{alloc_}; auto d_num_out = reinterpret_cast\u0026lt;unsigned long long*\u0026gt;( std::allocator_traits\u0026lt;temp_allocator_type\u0026gt;::allocate(temp_allocator, sizeof(unsigned long long))); CUCO_CUDA_TRY(cudaMemsetAsync(d_num_out, 0, sizeof(unsigned long long), stream)); detail::retrieve_all\u0026lt;block_size\u0026gt;\u0026lt;\u0026lt;\u0026lt;grid_size, block_size, 0, stream\u0026gt;\u0026gt;\u0026gt;( keys_out, values_out, submap_views_.data().get(), submaps_.size(), capacity, d_num_out, submap_cap_prefix_d.data().get(), empty_key_sentinel_, erased_key_sentinel_); size_t h_num_out; CUCO_CUDA_TRY( cudaMemcpyAsync(\u0026amp;h_num_out, d_num_out, sizeof(size_t), cudaMemcpyDeviceToHost, stream)); CUCO_CUDA_TRY(cudaStreamSynchronize(stream)); CUCO_CUDA_TRY(cudaFree(d_num_out)) return {keys_out + h_num_out, values_out + h_num_out}; } naive实现，一个全局atomic template \u0026lt;uint32_t block_size, typename OutputIt, typename viewT, typename PrefixT, typename Key\u0026gt; CUCO_KERNEL void retrieve_all(OutputIt keys_out, OutputIt values_out, viewT* submap_views, uint32_t num_submaps, uint64_t capacity, unsigned long long* d_num_out, PrefixT* prefix_sum, Key empty_key_sentinel, Key erased_key_sentinel) { auto tid = blockDim.x * blockIdx.x + threadIdx.x; auto stride = blockDim.x * gridDim.x; for (; tid \u0026lt; capacity; tid += stride) { uint32_t submap_idx = 0; uint32_t submap_offset = tid; // prefix_sum长度一般就10以内，不需要二分之类的操作 while (tid \u0026gt;= prefix_sum[submap_idx] \u0026amp;\u0026amp; submap_idx \u0026lt; num_submaps ) ++submap_idx; if (submap_idx \u0026gt; 0) { submap_offset = tid - prefix_sum[submap_idx - 1]; } auto const \u0026amp;current_slot = submap_views[submap_idx].get_slots()[submap_offset]; Key const existing_key = current_slot.first.load(cuda::std::memory_order_relaxed); auto const is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or cuco::detail::bitwise_compare(existing_key, erased_key_sentinel)); if (is_filled) { auto idx = atomicAdd(d_num_out, static_cast\u0026lt;unsigned long long\u0026gt;(1)); auto value = current_slot.second.load(cuda::std::memory_order_relaxed); keys_out[idx] = existing_key; values_out[idx] = value; } } } block内atomicAdd + 全局atomicAdd // 一个block内用一个__shared__ local_count表示这个block中predict为true的数量 // local_pos表示当前线程在该block内第几个predict为true template \u0026lt;uint32_t block_size, typename OutputIt, typename viewT, typename PrefixT, typename Key\u0026gt; CUCO_KERNEL void retrieve_all(OutputIt keys_out, OutputIt values_out, viewT* submap_views, uint32_t num_submaps, uint64_t capacity, unsigned long long* d_num_out, PrefixT* prefix_sum, Key empty_key_sentinel, Key erased_key_sentinel) { auto tid = blockDim.x * blockIdx.x + threadIdx.x; auto stride = blockDim.x * gridDim.x; __shared__ unsigned int local_count; if (threadIdx.x == 0) { local_count = 0; } __syncthreads(); for (; tid \u0026lt; capacity; tid += stride) { uint32_t submap_idx = 0; uint32_t submap_offset = tid; while (tid \u0026gt;= prefix_sum[submap_idx] \u0026amp;\u0026amp; submap_idx \u0026lt; num_submaps ) ++submap_idx; if (submap_idx \u0026gt; 0) { submap_offset = tid - prefix_sum[submap_idx - 1]; } auto const \u0026amp;current_slot = submap_views[submap_idx].get_slots()[submap_offset]; Key const existing_key = current_slot.first.load(cuda::std::memory_order_relaxed); auto const is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or cuco::detail::bitwise_compare(existing_key, erased_key_sentinel)); unsigned int local_pos = 0; if (is_filled) { local_pos = atomicAdd_block(\u0026amp;local_count, 1); } __syncthreads(); if (threadIdx.x == 0) { local_count = atomicAdd(d_num_out, local_count); } __syncthreads(); if (is_filled) { auto value = current_slot.second.load(cuda::std::memory_order_relaxed); keys_out[local_count + local_pos] = existing_key; values_out[local_count + local_pos] = value; } } } // 类似原理，但是用cub::BlockScan实现 template \u0026lt;uint32_t block_size, typename OutputIt, typename viewT, typename PrefixT, typename Key\u0026gt; CUCO_KERNEL void retrieve_all(OutputIt keys_out, OutputIt values_out, viewT* submap_views, uint32_t num_submaps, uint64_t capacity, unsigned long long* d_num_out, PrefixT* prefix_sum, Key empty_key_sentinel, Key erased_key_sentinel) { using BlockScan = cub::BlockScan\u0026lt;unsigned int, block_size\u0026gt;; // Shared memory __shared__ typename BlockScan::TempStorage scan_temp_storage; __shared__ unsigned int block_base; auto tid = blockDim.x * blockIdx.x + threadIdx.x; auto stride = blockDim.x * gridDim.x; for (; tid \u0026lt; capacity; tid += stride) { // Compute submap index and offset uint32_t submap_idx = 0; uint32_t submap_offset = tid; while (tid \u0026gt;= prefix_sum[submap_idx] \u0026amp;\u0026amp; submap_idx \u0026lt; num_submaps) ++submap_idx; if (submap_idx \u0026gt; 0) { submap_offset = tid - prefix_sum[submap_idx - 1]; } auto const\u0026amp; current_slot = submap_views[submap_idx].get_slots()[submap_offset]; Key const existing_key = current_slot.first.load(cuda::std::memory_order_relaxed); // Check key validity bool is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) || cuco::detail::bitwise_compare(existing_key, erased_key_sentinel)); // Perform block-wide exclusive scan to compute local write index unsigned int local_idx = 0; unsigned int total_valid = 0; BlockScan(scan_temp_storage).ExclusiveSum(is_filled ? 1u : 0u, local_idx, total_valid); // Block leader calculates global offset if (threadIdx.x == 0) { block_base = atomicAdd(d_num_out, total_valid); } __syncthreads(); if (is_filled) { auto value = current_slot.second.load(cuda::std::memory_order_relaxed); keys_out[block_base + local_idx] = existing_key; values_out[block_base + local_idx] = value; } } } 3. warp-aggregated atomics: warp(or cooperative group)粒度atomicAdd + block内atomicAdd+全局atomicAdd ```cpp template \u0026lt;uint32_t block_size, typename OutputIt, typename viewT, typename PrefixT, typename Key\u0026gt; CUCO_KERNEL void retrieve_all(OutputIt keys_out, OutputIt values_out, viewT* submap_views, uint32_t num_submaps, uint64_t capacity, unsigned long long* d_num_out, PrefixT* prefix_sum, Key empty_key_sentinel, Key erased_key_sentinel) { auto tid = blockDim.x * blockIdx.x + threadIdx.x; auto stride = blockDim.x * gridDim.x; __shared__ unsigned int block_count; __shared__ unsigned int block_base; if (threadIdx.x == 0) { block_count = 0; block_base = 0; } __syncthreads(); unsigned int local_idx = 0; for (; tid \u0026lt; capacity; tid += stride) { uint32_t submap_idx = 0; uint32_t submap_offset = tid; while (tid \u0026gt;= prefix_sum[submap_idx] \u0026amp;\u0026amp; submap_idx \u0026lt; num_submaps ) ++submap_idx; if (submap_idx \u0026gt; 0) { submap_offset = tid - prefix_sum[submap_idx - 1]; } auto const \u0026amp;current_slot = submap_views[submap_idx].get_slots()[submap_offset]; Key const existing_key = current_slot.first.load(cuda::std::memory_order_relaxed); auto const is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or cuco::detail::bitwise_compare(existing_key, erased_key_sentinel)); unsigned mask = __ballot_sync(0xffffffff, is_filled); int lane = threadIdx.x \u0026amp; 0x1f; int warp_prefix = __popc(mask \u0026amp; ((1u \u0026lt;\u0026lt; lane) - 1)); if (is_filled) local_idx = warp_prefix; unsigned int warp_vote = __popc(mask); unsigned int warp_base = 0; if (lane == 0 \u0026amp;\u0026amp; warp_vote) { warp_base = atomicAdd_block(\u0026amp;block_count, warp_vote); } warp_base = __shfl_sync(0xffffffff, warp_base, 0); __syncthreads(); if (threadIdx.x == 0) { block_base = atomicAdd(d_num_out, block_count); } __syncthreads(); if (is_filled) { auto value = current_slot.second.load(cuda::std::memory_order_relaxed); keys_out[block_base + warp_base + local_idx] = existing_key; values_out[block_base + warp_base + local_idx] = value; } } } // 类似的，但是用cooperative group实现,实测还是tile_size=32最快，和warp没区别 // 写起来更modern一点 template \u0026lt;uint32_t block_size, uint32_t tile_size, typename OutputIt, typename viewT, typename PrefixT, typename Key\u0026gt; CUCO_KERNEL void retrieve_all(OutputIt keys_out, OutputIt values_out, viewT* submap_views, uint32_t num_submaps, uint64_t capacity, unsigned long long* d_num_out, PrefixT* prefix_sum, Key empty_key_sentinel, Key erased_key_sentinel) { auto tile = cg::tiled_partition\u0026lt;tile_size\u0026gt;(cg::this_thread_block()); auto block = cg::this_thread_block(); auto tid = blockDim.x * blockIdx.x + threadIdx.x; auto stride = blockDim.x * gridDim.x; __shared__ unsigned int block_count; __shared__ unsigned int block_base; if (threadIdx.x == 0) { block_count = 0; block_base = 0; } block.sync(); for (; tid \u0026lt; capacity; tid += stride) { uint32_t submap_idx = 0; uint32_t submap_offset = tid; while (tid \u0026gt;= prefix_sum[submap_idx] \u0026amp;\u0026amp; submap_idx \u0026lt; num_submaps ) ++submap_idx; if (submap_idx \u0026gt; 0) { submap_offset = tid - prefix_sum[submap_idx - 1]; } auto const \u0026amp;current_slot = submap_views[submap_idx].get_slots()[submap_offset]; Key const existing_key = current_slot.first.load(cuda::std::memory_order_relaxed); auto const is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or cuco::detail::bitwise_compare(existing_key, erased_key_sentinel)); unsigned int tile_mask = tile.ballot(is_filled); unsigned int tile_rank = tile.thread_rank(); unsigned int tile_vote = __popc(tile_mask); unsigned int tile_prefix = __popc(tile_mask \u0026amp; ((1u \u0026lt;\u0026lt; tile_rank) - 1)); unsigned int tile_base = 0; if (tile_rank == 0 \u0026amp;\u0026amp; tile_mask) { tile_base = atomicAdd_block(\u0026amp;block_count, tile_vote); } tile_base = tile.shfl(tile_base, 0); block.sync(); if (block.thread_rank() == 0) { block_base = atomicAdd(d_num_out, block_count); } block.sync(); if (is_filled) { auto value = current_slot.second.load(cuda::std::memory_order_relaxed); keys_out[block_base + tile_base + tile_prefix] = existing_key; values_out[block_base + tile_base + tile_prefix] = value; } } } 实测2/3速度差不多，在插入1亿数据后（实际总cap达到2亿），\u0026lt;key, value\u0026gt;都是cuda::atomic\u0026lt;int64_t\u0026gt;的情况下，retrieve_all cost 3ms左右；\n",
  "keywords": [
    ["CUDA"]
  ],
  "articleBody": " still draft, to be updated\n实现了一个dynamic_map的retrieve_all 访存算子，经典过滤问题，predict为true的元素拷贝到out数组，重点是需要维护一个 atomic 的out index, https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/, 翻译：https://zhuanlan.zhihu.com/p/581078557\nhost端：\ntemplate \u003ctypename Key, typename Value, cuda::thread_scope Scope, typename Allocator\u003e template \u003ctypename KeyOut, typename ValueOut\u003e std::pair\u003cKeyOut, ValueOut\u003e dynamic_map\u003cKey, Value, Scope, Allocator\u003e::retrieve_all( KeyOut keys_out, ValueOut values_out, cudaStream_t stream) const { auto constexpr block_size = 128; auto constexpr stride = 1; auto const capacity = get_capacity(); auto grid_size = (capacity + stride * block_size - 1) / (stride * block_size); std::vector\u003csize_t\u003e submap_cap_prefix(submaps_.size()); std::inclusive_scan(submaps_.begin(), submaps_.end(), submap_cap_prefix.begin(), [](auto const\u0026 sum, auto const\u0026 submap) { return sum + submap-\u003eget_capacity(); }, (size_t)0); thrust::device_vector\u003csize_t\u003e submap_cap_prefix_d(submap_cap_prefix); // 复用alloc_（用于slots_的alloc_）会比直接cudaMalloc快一个数量级，不需要重新分配内存 // 单纯cudaMalloc会触发GPU driver/runtime 的 allocation 初始化、页表建立等 using temp_allocator_type = typename std::allocator_traits\u003cAllocator\u003e::template rebind_alloc\u003cchar\u003e; auto temp_allocator = temp_allocator_type{alloc_}; auto d_num_out = reinterpret_cast\u003cunsigned long long*\u003e( std::allocator_traits\u003ctemp_allocator_type\u003e::allocate(temp_allocator, sizeof(unsigned long long))); CUCO_CUDA_TRY(cudaMemsetAsync(d_num_out, 0, sizeof(unsigned long long), stream)); detail::retrieve_all\u003cblock_size\u003e\u003c\u003c\u003cgrid_size, block_size, 0, stream\u003e\u003e\u003e( keys_out, values_out, submap_views_.data().get(), submaps_.size(), capacity, d_num_out, submap_cap_prefix_d.data().get(), empty_key_sentinel_, erased_key_sentinel_); size_t h_num_out; CUCO_CUDA_TRY( cudaMemcpyAsync(\u0026h_num_out, d_num_out, sizeof(size_t), cudaMemcpyDeviceToHost, stream)); CUCO_CUDA_TRY(cudaStreamSynchronize(stream)); CUCO_CUDA_TRY(cudaFree(d_num_out)) return {keys_out + h_num_out, values_out + h_num_out}; } naive实现，一个全局atomic template \u003cuint32_t block_size, typename OutputIt, typename viewT, typename PrefixT, typename Key\u003e CUCO_KERNEL void retrieve_all(OutputIt keys_out, OutputIt values_out, viewT* submap_views, uint32_t num_submaps, uint64_t capacity, unsigned long long* d_num_out, PrefixT* prefix_sum, Key empty_key_sentinel, Key erased_key_sentinel) { auto tid = blockDim.x * blockIdx.x + threadIdx.x; auto stride = blockDim.x * gridDim.x; for (; tid \u003c capacity; tid += stride) { uint32_t submap_idx = 0; uint32_t submap_offset = tid; // prefix_sum长度一般就10以内，不需要二分之类的操作 while (tid \u003e= prefix_sum[submap_idx] \u0026\u0026 submap_idx \u003c num_submaps ) ++submap_idx; if (submap_idx \u003e 0) { submap_offset = tid - prefix_sum[submap_idx - 1]; } auto const \u0026current_slot = submap_views[submap_idx].get_slots()[submap_offset]; Key const existing_key = current_slot.first.load(cuda::std::memory_order_relaxed); auto const is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or cuco::detail::bitwise_compare(existing_key, erased_key_sentinel)); if (is_filled) { auto idx = atomicAdd(d_num_out, static_cast\u003cunsigned long long\u003e(1)); auto value = current_slot.second.load(cuda::std::memory_order_relaxed); keys_out[idx] = existing_key; values_out[idx] = value; } } } block内atomicAdd + 全局atomicAdd // 一个block内用一个__shared__ local_count表示这个block中predict为true的数量 // local_pos表示当前线程在该block内第几个predict为true template \u003cuint32_t block_size, typename OutputIt, typename viewT, typename PrefixT, typename Key\u003e CUCO_KERNEL void retrieve_all(OutputIt keys_out, OutputIt values_out, viewT* submap_views, uint32_t num_submaps, uint64_t capacity, unsigned long long* d_num_out, PrefixT* prefix_sum, Key empty_key_sentinel, Key erased_key_sentinel) { auto tid = blockDim.x * blockIdx.x + threadIdx.x; auto stride = blockDim.x * gridDim.x; __shared__ unsigned int local_count; if (threadIdx.x == 0) { local_count = 0; } __syncthreads(); for (; tid \u003c capacity; tid += stride) { uint32_t submap_idx = 0; uint32_t submap_offset = tid; while (tid \u003e= prefix_sum[submap_idx] \u0026\u0026 submap_idx \u003c num_submaps ) ++submap_idx; if (submap_idx \u003e 0) { submap_offset = tid - prefix_sum[submap_idx - 1]; } auto const \u0026current_slot = submap_views[submap_idx].get_slots()[submap_offset]; Key const existing_key = current_slot.first.load(cuda::std::memory_order_relaxed); auto const is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or cuco::detail::bitwise_compare(existing_key, erased_key_sentinel)); unsigned int local_pos = 0; if (is_filled) { local_pos = atomicAdd_block(\u0026local_count, 1); } __syncthreads(); if (threadIdx.x == 0) { local_count = atomicAdd(d_num_out, local_count); } __syncthreads(); if (is_filled) { auto value = current_slot.second.load(cuda::std::memory_order_relaxed); keys_out[local_count + local_pos] = existing_key; values_out[local_count + local_pos] = value; } } } // 类似原理，但是用cub::BlockScan实现 template \u003cuint32_t block_size, typename OutputIt, typename viewT, typename PrefixT, typename Key\u003e CUCO_KERNEL void retrieve_all(OutputIt keys_out, OutputIt values_out, viewT* submap_views, uint32_t num_submaps, uint64_t capacity, unsigned long long* d_num_out, PrefixT* prefix_sum, Key empty_key_sentinel, Key erased_key_sentinel) { using BlockScan = cub::BlockScan\u003cunsigned int, block_size\u003e; // Shared memory __shared__ typename BlockScan::TempStorage scan_temp_storage; __shared__ unsigned int block_base; auto tid = blockDim.x * blockIdx.x + threadIdx.x; auto stride = blockDim.x * gridDim.x; for (; tid \u003c capacity; tid += stride) { // Compute submap index and offset uint32_t submap_idx = 0; uint32_t submap_offset = tid; while (tid \u003e= prefix_sum[submap_idx] \u0026\u0026 submap_idx \u003c num_submaps) ++submap_idx; if (submap_idx \u003e 0) { submap_offset = tid - prefix_sum[submap_idx - 1]; } auto const\u0026 current_slot = submap_views[submap_idx].get_slots()[submap_offset]; Key const existing_key = current_slot.first.load(cuda::std::memory_order_relaxed); // Check key validity bool is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) || cuco::detail::bitwise_compare(existing_key, erased_key_sentinel)); // Perform block-wide exclusive scan to compute local write index unsigned int local_idx = 0; unsigned int total_valid = 0; BlockScan(scan_temp_storage).ExclusiveSum(is_filled ? 1u : 0u, local_idx, total_valid); // Block leader calculates global offset if (threadIdx.x == 0) { block_base = atomicAdd(d_num_out, total_valid); } __syncthreads(); if (is_filled) { auto value = current_slot.second.load(cuda::std::memory_order_relaxed); keys_out[block_base + local_idx] = existing_key; values_out[block_base + local_idx] = value; } } } 3. warp-aggregated atomics: warp(or cooperative group)粒度atomicAdd + block内atomicAdd+全局atomicAdd ```cpp template \u003cuint32_t block_size, typename OutputIt, typename viewT, typename PrefixT, typename Key\u003e CUCO_KERNEL void retrieve_all(OutputIt keys_out, OutputIt values_out, viewT* submap_views, uint32_t num_submaps, uint64_t capacity, unsigned long long* d_num_out, PrefixT* prefix_sum, Key empty_key_sentinel, Key erased_key_sentinel) { auto tid = blockDim.x * blockIdx.x + threadIdx.x; auto stride = blockDim.x * gridDim.x; __shared__ unsigned int block_count; __shared__ unsigned int block_base; if (threadIdx.x == 0) { block_count = 0; block_base = 0; } __syncthreads(); unsigned int local_idx = 0; for (; tid \u003c capacity; tid += stride) { uint32_t submap_idx = 0; uint32_t submap_offset = tid; while (tid \u003e= prefix_sum[submap_idx] \u0026\u0026 submap_idx \u003c num_submaps ) ++submap_idx; if (submap_idx \u003e 0) { submap_offset = tid - prefix_sum[submap_idx - 1]; } auto const \u0026current_slot = submap_views[submap_idx].get_slots()[submap_offset]; Key const existing_key = current_slot.first.load(cuda::std::memory_order_relaxed); auto const is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or cuco::detail::bitwise_compare(existing_key, erased_key_sentinel)); unsigned mask = __ballot_sync(0xffffffff, is_filled); int lane = threadIdx.x \u0026 0x1f; int warp_prefix = __popc(mask \u0026 ((1u \u003c\u003c lane) - 1)); if (is_filled) local_idx = warp_prefix; unsigned int warp_vote = __popc(mask); unsigned int warp_base = 0; if (lane == 0 \u0026\u0026 warp_vote) { warp_base = atomicAdd_block(\u0026block_count, warp_vote); } warp_base = __shfl_sync(0xffffffff, warp_base, 0); __syncthreads(); if (threadIdx.x == 0) { block_base = atomicAdd(d_num_out, block_count); } __syncthreads(); if (is_filled) { auto value = current_slot.second.load(cuda::std::memory_order_relaxed); keys_out[block_base + warp_base + local_idx] = existing_key; values_out[block_base + warp_base + local_idx] = value; } } } // 类似的，但是用cooperative group实现,实测还是tile_size=32最快，和warp没区别 // 写起来更modern一点 template \u003cuint32_t block_size, uint32_t tile_size, typename OutputIt, typename viewT, typename PrefixT, typename Key\u003e CUCO_KERNEL void retrieve_all(OutputIt keys_out, OutputIt values_out, viewT* submap_views, uint32_t num_submaps, uint64_t capacity, unsigned long long* d_num_out, PrefixT* prefix_sum, Key empty_key_sentinel, Key erased_key_sentinel) { auto tile = cg::tiled_partition\u003ctile_size\u003e(cg::this_thread_block()); auto block = cg::this_thread_block(); auto tid = blockDim.x * blockIdx.x + threadIdx.x; auto stride = blockDim.x * gridDim.x; __shared__ unsigned int block_count; __shared__ unsigned int block_base; if (threadIdx.x == 0) { block_count = 0; block_base = 0; } block.sync(); for (; tid \u003c capacity; tid += stride) { uint32_t submap_idx = 0; uint32_t submap_offset = tid; while (tid \u003e= prefix_sum[submap_idx] \u0026\u0026 submap_idx \u003c num_submaps ) ++submap_idx; if (submap_idx \u003e 0) { submap_offset = tid - prefix_sum[submap_idx - 1]; } auto const \u0026current_slot = submap_views[submap_idx].get_slots()[submap_offset]; Key const existing_key = current_slot.first.load(cuda::std::memory_order_relaxed); auto const is_filled = not(cuco::detail::bitwise_compare(existing_key, empty_key_sentinel) or cuco::detail::bitwise_compare(existing_key, erased_key_sentinel)); unsigned int tile_mask = tile.ballot(is_filled); unsigned int tile_rank = tile.thread_rank(); unsigned int tile_vote = __popc(tile_mask); unsigned int tile_prefix = __popc(tile_mask \u0026 ((1u \u003c\u003c tile_rank) - 1)); unsigned int tile_base = 0; if (tile_rank == 0 \u0026\u0026 tile_mask) { tile_base = atomicAdd_block(\u0026block_count, tile_vote); } tile_base = tile.shfl(tile_base, 0); block.sync(); if (block.thread_rank() == 0) { block_base = atomicAdd(d_num_out, block_count); } block.sync(); if (is_filled) { auto value = current_slot.second.load(cuda::std::memory_order_relaxed); keys_out[block_base + tile_base + tile_prefix] = existing_key; values_out[block_base + tile_base + tile_prefix] = value; } } } 实测2/3速度差不多，在插入1亿数据后（实际总cap达到2亿），都是cuda::atomic的情况下，retrieve_all cost 3ms左右；\n",
  "wordCount" : "1121",
  "inLanguage": "en",
  "datePublished": "2025-04-28T00:00:00Z",
  "dateModified": "2025-04-28T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://caaatch22.github.io/posts/cuco/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Mingjie's Home",
    "logo": {
      "@type": "ImageObject",
      "url": "https://caaatch22.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://caaatch22.github.io/" accesskey="h" title="Mingjie&#39;s Home (Alt + H)">Mingjie&#39;s Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://caaatch22.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://caaatch22.github.io/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://caaatch22.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://caaatch22.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://caaatch22.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://caaatch22.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      cuCollections
    </h1>
    <div class="post-meta"><span title='2025-04-28 00:00:00 +0000 UTC'>April 28, 2025</span>&nbsp;·&nbsp;6 min

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e5%ae%9e%e7%8e%b0%e4%ba%86%e4%b8%80%e4%b8%aadynamic_map%e7%9a%84retrieve_all" aria-label="实现了一个dynamic_map的retrieve_all">实现了一个dynamic_map的retrieve_all</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><blockquote>
<p>still draft, to be updated</p>
</blockquote>
<h3 id="实现了一个dynamic_map的retrieve_all">实现了一个dynamic_map的retrieve_all<a hidden class="anchor" aria-hidden="true" href="#实现了一个dynamic_map的retrieve_all">#</a></h3>
<blockquote>
<p>访存算子，经典过滤问题，predict为true的元素拷贝到out数组，重点是需要维护一个 atomic 的out index, <a href="https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/">https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/</a>, 翻译：https://zhuanlan.zhihu.com/p/581078557</p>
</blockquote>
<p>host端：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">template</span> <span style="color:#f92672">&lt;</span><span style="color:#66d9ef">typename</span> Key, <span style="color:#66d9ef">typename</span> Value, cuda<span style="color:#f92672">::</span>thread_scope Scope, <span style="color:#66d9ef">typename</span> Allocator<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">template</span> <span style="color:#f92672">&lt;</span><span style="color:#66d9ef">typename</span> KeyOut, <span style="color:#66d9ef">typename</span> ValueOut<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>std<span style="color:#f92672">::</span>pair<span style="color:#f92672">&lt;</span>KeyOut, ValueOut<span style="color:#f92672">&gt;</span> dynamic_map<span style="color:#f92672">&lt;</span>Key, Value, Scope, Allocator<span style="color:#f92672">&gt;::</span>retrieve_all(
</span></span><span style="display:flex;"><span>  KeyOut keys_out, ValueOut values_out, cudaStream_t stream) <span style="color:#66d9ef">const</span>
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">constexpr</span> block_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">constexpr</span> stride     <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span> capacity       <span style="color:#f92672">=</span> get_capacity();
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> grid_size            <span style="color:#f92672">=</span> (capacity <span style="color:#f92672">+</span> stride <span style="color:#f92672">*</span> block_size <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> (stride <span style="color:#f92672">*</span> block_size);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>size_t<span style="color:#f92672">&gt;</span> submap_cap_prefix(submaps_.size());
</span></span><span style="display:flex;"><span>  std<span style="color:#f92672">::</span>inclusive_scan(submaps_.begin(), submaps_.end(), submap_cap_prefix.begin(),
</span></span><span style="display:flex;"><span>                      [](<span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span><span style="color:#f92672">&amp;</span> sum, <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span><span style="color:#f92672">&amp;</span> submap) { <span style="color:#66d9ef">return</span> sum <span style="color:#f92672">+</span> submap<span style="color:#f92672">-&gt;</span>get_capacity(); }, 
</span></span><span style="display:flex;"><span>                      (size_t)<span style="color:#ae81ff">0</span>);
</span></span><span style="display:flex;"><span>  thrust<span style="color:#f92672">::</span>device_vector<span style="color:#f92672">&lt;</span>size_t<span style="color:#f92672">&gt;</span> submap_cap_prefix_d(submap_cap_prefix);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// 复用alloc_（用于slots_的alloc_）会比直接cudaMalloc快一个数量级，不需要重新分配内存
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#75715e">// 单纯cudaMalloc会触发GPU driver/runtime 的 allocation 初始化、页表建立等
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">using</span> temp_allocator_type <span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">typename</span> std<span style="color:#f92672">::</span>allocator_traits<span style="color:#f92672">&lt;</span>Allocator<span style="color:#f92672">&gt;::</span><span style="color:#66d9ef">template</span> rebind_alloc<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">char</span><span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> temp_allocator <span style="color:#f92672">=</span> temp_allocator_type{alloc_};
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> d_num_out      <span style="color:#f92672">=</span> <span style="color:#66d9ef">reinterpret_cast</span><span style="color:#f92672">&lt;</span><span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">long</span> <span style="color:#66d9ef">long</span><span style="color:#f92672">*&gt;</span>(
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>allocator_traits<span style="color:#f92672">&lt;</span>temp_allocator_type<span style="color:#f92672">&gt;::</span>allocate(temp_allocator, <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">long</span> <span style="color:#66d9ef">long</span>)));
</span></span><span style="display:flex;"><span>  CUCO_CUDA_TRY(cudaMemsetAsync(d_num_out, <span style="color:#ae81ff">0</span>, <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">long</span> <span style="color:#66d9ef">long</span>), stream));
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  detail<span style="color:#f92672">::</span>retrieve_all<span style="color:#f92672">&lt;</span>block_size<span style="color:#f92672">&gt;&lt;&lt;&lt;</span>grid_size, block_size, <span style="color:#ae81ff">0</span>, stream<span style="color:#f92672">&gt;&gt;&gt;</span>(
</span></span><span style="display:flex;"><span>    keys_out, values_out, submap_views_.data().get(), submaps_.size(), 
</span></span><span style="display:flex;"><span>    capacity, d_num_out, submap_cap_prefix_d.data().get(), 
</span></span><span style="display:flex;"><span>    empty_key_sentinel_, erased_key_sentinel_);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  size_t h_num_out;
</span></span><span style="display:flex;"><span>  CUCO_CUDA_TRY(
</span></span><span style="display:flex;"><span>    cudaMemcpyAsync(<span style="color:#f92672">&amp;</span>h_num_out, d_num_out, <span style="color:#66d9ef">sizeof</span>(size_t), cudaMemcpyDeviceToHost, stream));
</span></span><span style="display:flex;"><span>  CUCO_CUDA_TRY(cudaStreamSynchronize(stream));
</span></span><span style="display:flex;"><span>  CUCO_CUDA_TRY(cudaFree(d_num_out))
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> {keys_out <span style="color:#f92672">+</span> h_num_out, values_out <span style="color:#f92672">+</span> h_num_out};
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><ol>
<li>naive实现，一个全局atomic</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">template</span> <span style="color:#f92672">&lt;</span><span style="color:#66d9ef">uint32_t</span> block_size,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> OutputIt,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> viewT,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> PrefixT,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> Key<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>CUCO_KERNEL <span style="color:#66d9ef">void</span> retrieve_all(OutputIt keys_out,
</span></span><span style="display:flex;"><span>                              OutputIt values_out,
</span></span><span style="display:flex;"><span>                              viewT<span style="color:#f92672">*</span> submap_views,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">uint32_t</span> num_submaps,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">uint64_t</span> capacity,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">long</span> <span style="color:#66d9ef">long</span><span style="color:#f92672">*</span> d_num_out,
</span></span><span style="display:flex;"><span>                              PrefixT<span style="color:#f92672">*</span> prefix_sum,
</span></span><span style="display:flex;"><span>                              Key empty_key_sentinel,
</span></span><span style="display:flex;"><span>                              Key erased_key_sentinel)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> tid    <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> blockIdx.x <span style="color:#f92672">+</span> threadIdx.x;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> stride <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> gridDim.x;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> (; tid <span style="color:#f92672">&lt;</span> capacity; tid <span style="color:#f92672">+=</span> stride) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">uint32_t</span> submap_idx <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">uint32_t</span> submap_offset <span style="color:#f92672">=</span> tid;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// prefix_sum长度一般就10以内，不需要二分之类的操作
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">while</span> (tid <span style="color:#f92672">&gt;=</span> prefix_sum[submap_idx] <span style="color:#f92672">&amp;&amp;</span> submap_idx <span style="color:#f92672">&lt;</span> num_submaps ) <span style="color:#f92672">++</span>submap_idx;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (submap_idx <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>      submap_offset <span style="color:#f92672">=</span> tid <span style="color:#f92672">-</span> prefix_sum[submap_idx <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>];
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span> <span style="color:#f92672">&amp;</span>current_slot  <span style="color:#f92672">=</span> submap_views[submap_idx].get_slots()[submap_offset];
</span></span><span style="display:flex;"><span>    Key <span style="color:#66d9ef">const</span> existing_key    <span style="color:#f92672">=</span> current_slot.first.load(cuda<span style="color:#f92672">::</span>std<span style="color:#f92672">::</span>memory_order_relaxed);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span> is_filled <span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>      not(cuco<span style="color:#f92672">::</span>detail<span style="color:#f92672">::</span>bitwise_compare(existing_key, empty_key_sentinel) or
</span></span><span style="display:flex;"><span>          cuco<span style="color:#f92672">::</span>detail<span style="color:#f92672">::</span>bitwise_compare(existing_key, erased_key_sentinel));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (is_filled) {
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">auto</span> idx   <span style="color:#f92672">=</span> atomicAdd(d_num_out, <span style="color:#66d9ef">static_cast</span><span style="color:#f92672">&lt;</span><span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">long</span> <span style="color:#66d9ef">long</span><span style="color:#f92672">&gt;</span>(<span style="color:#ae81ff">1</span>));
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">auto</span> value <span style="color:#f92672">=</span> current_slot.second.load(cuda<span style="color:#f92672">::</span>std<span style="color:#f92672">::</span>memory_order_relaxed);
</span></span><span style="display:flex;"><span>      keys_out[idx]    <span style="color:#f92672">=</span> existing_key;
</span></span><span style="display:flex;"><span>      values_out[idx]  <span style="color:#f92672">=</span> value;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><ol start="2">
<li>block内atomicAdd + 全局atomicAdd</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">// 一个block内用一个__shared__ local_count表示这个block中predict为true的数量
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// local_pos表示当前线程在该block内第几个predict为true
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">template</span> <span style="color:#f92672">&lt;</span><span style="color:#66d9ef">uint32_t</span> block_size,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> OutputIt,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> viewT,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> PrefixT,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> Key<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>CUCO_KERNEL <span style="color:#66d9ef">void</span> retrieve_all(OutputIt keys_out,
</span></span><span style="display:flex;"><span>                              OutputIt values_out,
</span></span><span style="display:flex;"><span>                              viewT<span style="color:#f92672">*</span> submap_views,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">uint32_t</span> num_submaps,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">uint64_t</span> capacity,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">long</span> <span style="color:#66d9ef">long</span><span style="color:#f92672">*</span> d_num_out,
</span></span><span style="display:flex;"><span>                              PrefixT<span style="color:#f92672">*</span> prefix_sum,
</span></span><span style="display:flex;"><span>                              Key empty_key_sentinel,
</span></span><span style="display:flex;"><span>                              Key erased_key_sentinel)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> tid    <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> blockIdx.x <span style="color:#f92672">+</span> threadIdx.x;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> stride <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> gridDim.x;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  __shared__ <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> local_count;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> (threadIdx.x <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>    local_count <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  __syncthreads();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> (; tid <span style="color:#f92672">&lt;</span> capacity; tid <span style="color:#f92672">+=</span> stride) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">uint32_t</span> submap_idx <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">uint32_t</span> submap_offset <span style="color:#f92672">=</span> tid;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> (tid <span style="color:#f92672">&gt;=</span> prefix_sum[submap_idx] <span style="color:#f92672">&amp;&amp;</span> submap_idx <span style="color:#f92672">&lt;</span> num_submaps ) <span style="color:#f92672">++</span>submap_idx;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (submap_idx <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>      submap_offset <span style="color:#f92672">=</span> tid <span style="color:#f92672">-</span> prefix_sum[submap_idx <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>];
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span> <span style="color:#f92672">&amp;</span>current_slot  <span style="color:#f92672">=</span> submap_views[submap_idx].get_slots()[submap_offset];
</span></span><span style="display:flex;"><span>    Key <span style="color:#66d9ef">const</span> existing_key    <span style="color:#f92672">=</span> current_slot.first.load(cuda<span style="color:#f92672">::</span>std<span style="color:#f92672">::</span>memory_order_relaxed);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span> is_filled <span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>      not(cuco<span style="color:#f92672">::</span>detail<span style="color:#f92672">::</span>bitwise_compare(existing_key, empty_key_sentinel) or
</span></span><span style="display:flex;"><span>          cuco<span style="color:#f92672">::</span>detail<span style="color:#f92672">::</span>bitwise_compare(existing_key, erased_key_sentinel));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> local_pos <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (is_filled) {
</span></span><span style="display:flex;"><span>      local_pos <span style="color:#f92672">=</span> atomicAdd_block(<span style="color:#f92672">&amp;</span>local_count, <span style="color:#ae81ff">1</span>);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    __syncthreads();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (threadIdx.x <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>      local_count <span style="color:#f92672">=</span> atomicAdd(d_num_out, local_count);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    __syncthreads();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (is_filled) {
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">auto</span> value <span style="color:#f92672">=</span> current_slot.second.load(cuda<span style="color:#f92672">::</span>std<span style="color:#f92672">::</span>memory_order_relaxed);
</span></span><span style="display:flex;"><span>      keys_out[local_count <span style="color:#f92672">+</span> local_pos]    <span style="color:#f92672">=</span> existing_key;
</span></span><span style="display:flex;"><span>      values_out[local_count <span style="color:#f92672">+</span> local_pos]  <span style="color:#f92672">=</span> value;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// 类似原理，但是用cub::BlockScan实现
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">template</span> <span style="color:#f92672">&lt;</span><span style="color:#66d9ef">uint32_t</span> block_size,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> OutputIt,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> viewT,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> PrefixT,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> Key<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>CUCO_KERNEL <span style="color:#66d9ef">void</span> retrieve_all(OutputIt keys_out,
</span></span><span style="display:flex;"><span>                              OutputIt values_out,
</span></span><span style="display:flex;"><span>                              viewT<span style="color:#f92672">*</span> submap_views,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">uint32_t</span> num_submaps,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">uint64_t</span> capacity,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">long</span> <span style="color:#66d9ef">long</span><span style="color:#f92672">*</span> d_num_out,
</span></span><span style="display:flex;"><span>                              PrefixT<span style="color:#f92672">*</span> prefix_sum,
</span></span><span style="display:flex;"><span>                              Key empty_key_sentinel,
</span></span><span style="display:flex;"><span>                              Key erased_key_sentinel)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">using</span> BlockScan <span style="color:#f92672">=</span> cub<span style="color:#f92672">::</span>BlockScan<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span>, block_size<span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// Shared memory
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  __shared__ <span style="color:#66d9ef">typename</span> BlockScan<span style="color:#f92672">::</span>TempStorage scan_temp_storage;
</span></span><span style="display:flex;"><span>  __shared__ <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> block_base;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> tid    <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> blockIdx.x <span style="color:#f92672">+</span> threadIdx.x;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> stride <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> gridDim.x;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> (; tid <span style="color:#f92672">&lt;</span> capacity; tid <span style="color:#f92672">+=</span> stride) {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// Compute submap index and offset
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">uint32_t</span> submap_idx <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">uint32_t</span> submap_offset <span style="color:#f92672">=</span> tid;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> (tid <span style="color:#f92672">&gt;=</span> prefix_sum[submap_idx] <span style="color:#f92672">&amp;&amp;</span> submap_idx <span style="color:#f92672">&lt;</span> num_submaps) <span style="color:#f92672">++</span>submap_idx;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (submap_idx <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>      submap_offset <span style="color:#f92672">=</span> tid <span style="color:#f92672">-</span> prefix_sum[submap_idx <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>];
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span><span style="color:#f92672">&amp;</span> current_slot <span style="color:#f92672">=</span> submap_views[submap_idx].get_slots()[submap_offset];
</span></span><span style="display:flex;"><span>    Key <span style="color:#66d9ef">const</span> existing_key   <span style="color:#f92672">=</span> current_slot.first.load(cuda<span style="color:#f92672">::</span>std<span style="color:#f92672">::</span>memory_order_relaxed);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// Check key validity
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">bool</span> is_filled <span style="color:#f92672">=</span> not(cuco<span style="color:#f92672">::</span>detail<span style="color:#f92672">::</span>bitwise_compare(existing_key, empty_key_sentinel) <span style="color:#f92672">||</span>
</span></span><span style="display:flex;"><span>                         cuco<span style="color:#f92672">::</span>detail<span style="color:#f92672">::</span>bitwise_compare(existing_key, erased_key_sentinel));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// Perform block-wide exclusive scan to compute local write index
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> local_idx <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> total_valid <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    BlockScan(scan_temp_storage).ExclusiveSum(is_filled <span style="color:#f92672">?</span> <span style="color:#ae81ff">1u</span> <span style="color:#f92672">:</span> <span style="color:#ae81ff">0u</span>, local_idx, total_valid);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// Block leader calculates global offset
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">if</span> (threadIdx.x <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>      block_base <span style="color:#f92672">=</span> atomicAdd(d_num_out, total_valid);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    __syncthreads();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (is_filled) {
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">auto</span> value <span style="color:#f92672">=</span> current_slot.second.load(cuda<span style="color:#f92672">::</span>std<span style="color:#f92672">::</span>memory_order_relaxed);
</span></span><span style="display:flex;"><span>      keys_out[block_base <span style="color:#f92672">+</span> local_idx]   <span style="color:#f92672">=</span> existing_key;
</span></span><span style="display:flex;"><span>      values_out[block_base <span style="color:#f92672">+</span> local_idx] <span style="color:#f92672">=</span> value;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3.</span> warp<span style="color:#f92672">-</span>aggregated atomics: warp(or cooperative group)<span style="color:#960050;background-color:#1e0010">粒度</span>atomicAdd <span style="color:#f92672">+</span> block内atomicAdd<span style="color:#f92672">+</span><span style="color:#960050;background-color:#1e0010">全局</span>atomicAdd
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">```</span>cpp
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">template</span> <span style="color:#f92672">&lt;</span><span style="color:#66d9ef">uint32_t</span> block_size,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> OutputIt,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> viewT,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> PrefixT,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> Key<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>CUCO_KERNEL <span style="color:#66d9ef">void</span> retrieve_all(OutputIt keys_out,
</span></span><span style="display:flex;"><span>                              OutputIt values_out,
</span></span><span style="display:flex;"><span>                              viewT<span style="color:#f92672">*</span> submap_views,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">uint32_t</span> num_submaps,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">uint64_t</span> capacity,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">long</span> <span style="color:#66d9ef">long</span><span style="color:#f92672">*</span> d_num_out,
</span></span><span style="display:flex;"><span>                              PrefixT<span style="color:#f92672">*</span> prefix_sum,
</span></span><span style="display:flex;"><span>                              Key empty_key_sentinel,
</span></span><span style="display:flex;"><span>                              Key erased_key_sentinel)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> tid    <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> blockIdx.x <span style="color:#f92672">+</span> threadIdx.x;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> stride <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> gridDim.x;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  __shared__ <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> block_count;
</span></span><span style="display:flex;"><span>  __shared__ <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> block_base;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> (threadIdx.x <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>    block_count <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    block_base <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  __syncthreads();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> local_idx <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> (; tid <span style="color:#f92672">&lt;</span> capacity; tid <span style="color:#f92672">+=</span> stride) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">uint32_t</span> submap_idx <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">uint32_t</span> submap_offset <span style="color:#f92672">=</span> tid;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> (tid <span style="color:#f92672">&gt;=</span> prefix_sum[submap_idx] <span style="color:#f92672">&amp;&amp;</span> submap_idx <span style="color:#f92672">&lt;</span> num_submaps ) <span style="color:#f92672">++</span>submap_idx;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (submap_idx <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>      submap_offset <span style="color:#f92672">=</span> tid <span style="color:#f92672">-</span> prefix_sum[submap_idx <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>];
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span> <span style="color:#f92672">&amp;</span>current_slot  <span style="color:#f92672">=</span> submap_views[submap_idx].get_slots()[submap_offset];
</span></span><span style="display:flex;"><span>    Key <span style="color:#66d9ef">const</span> existing_key    <span style="color:#f92672">=</span> current_slot.first.load(cuda<span style="color:#f92672">::</span>std<span style="color:#f92672">::</span>memory_order_relaxed);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span> is_filled <span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>      not(cuco<span style="color:#f92672">::</span>detail<span style="color:#f92672">::</span>bitwise_compare(existing_key, empty_key_sentinel) or
</span></span><span style="display:flex;"><span>          cuco<span style="color:#f92672">::</span>detail<span style="color:#f92672">::</span>bitwise_compare(existing_key, erased_key_sentinel));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> mask <span style="color:#f92672">=</span> __ballot_sync(<span style="color:#ae81ff">0xffffffff</span>, is_filled);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> lane <span style="color:#f92672">=</span> threadIdx.x <span style="color:#f92672">&amp;</span> <span style="color:#ae81ff">0x1f</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> warp_prefix <span style="color:#f92672">=</span> __popc(mask <span style="color:#f92672">&amp;</span> ((<span style="color:#ae81ff">1u</span> <span style="color:#f92672">&lt;&lt;</span> lane) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>));
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (is_filled) local_idx <span style="color:#f92672">=</span> warp_prefix;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> warp_vote <span style="color:#f92672">=</span> __popc(mask);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> warp_base <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (lane <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&amp;&amp;</span> warp_vote) {
</span></span><span style="display:flex;"><span>      warp_base <span style="color:#f92672">=</span> atomicAdd_block(<span style="color:#f92672">&amp;</span>block_count, warp_vote);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    warp_base <span style="color:#f92672">=</span> __shfl_sync(<span style="color:#ae81ff">0xffffffff</span>, warp_base, <span style="color:#ae81ff">0</span>);
</span></span><span style="display:flex;"><span>    __syncthreads();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (threadIdx.x <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>      block_base <span style="color:#f92672">=</span> atomicAdd(d_num_out, block_count);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    __syncthreads();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (is_filled) {
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">auto</span> value <span style="color:#f92672">=</span> current_slot.second.load(cuda<span style="color:#f92672">::</span>std<span style="color:#f92672">::</span>memory_order_relaxed);
</span></span><span style="display:flex;"><span>      keys_out[block_base <span style="color:#f92672">+</span> warp_base <span style="color:#f92672">+</span> local_idx]    <span style="color:#f92672">=</span> existing_key;
</span></span><span style="display:flex;"><span>      values_out[block_base <span style="color:#f92672">+</span> warp_base <span style="color:#f92672">+</span> local_idx]  <span style="color:#f92672">=</span> value;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// 类似的，但是用cooperative group实现,实测还是tile_size=32最快，和warp没区别
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// 写起来更modern一点
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">template</span> <span style="color:#f92672">&lt;</span><span style="color:#66d9ef">uint32_t</span> block_size,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">uint32_t</span> tile_size,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> OutputIt,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> viewT,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> PrefixT,
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">typename</span> Key<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>CUCO_KERNEL <span style="color:#66d9ef">void</span> retrieve_all(OutputIt keys_out,
</span></span><span style="display:flex;"><span>                              OutputIt values_out,
</span></span><span style="display:flex;"><span>                              viewT<span style="color:#f92672">*</span> submap_views,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">uint32_t</span> num_submaps,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">uint64_t</span> capacity,
</span></span><span style="display:flex;"><span>                              <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">long</span> <span style="color:#66d9ef">long</span><span style="color:#f92672">*</span> d_num_out,
</span></span><span style="display:flex;"><span>                              PrefixT<span style="color:#f92672">*</span> prefix_sum,
</span></span><span style="display:flex;"><span>                              Key empty_key_sentinel,
</span></span><span style="display:flex;"><span>                              Key erased_key_sentinel)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> tile   <span style="color:#f92672">=</span> cg<span style="color:#f92672">::</span>tiled_partition<span style="color:#f92672">&lt;</span>tile_size<span style="color:#f92672">&gt;</span>(cg<span style="color:#f92672">::</span>this_thread_block());
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> block  <span style="color:#f92672">=</span> cg<span style="color:#f92672">::</span>this_thread_block();
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> tid    <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> blockIdx.x <span style="color:#f92672">+</span> threadIdx.x;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">auto</span> stride <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> gridDim.x;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  __shared__ <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> block_count;
</span></span><span style="display:flex;"><span>  __shared__ <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> block_base;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> (threadIdx.x <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>    block_count <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    block_base <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  block.sync();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> (; tid <span style="color:#f92672">&lt;</span> capacity; tid <span style="color:#f92672">+=</span> stride) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">uint32_t</span> submap_idx <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">uint32_t</span> submap_offset <span style="color:#f92672">=</span> tid;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> (tid <span style="color:#f92672">&gt;=</span> prefix_sum[submap_idx] <span style="color:#f92672">&amp;&amp;</span> submap_idx <span style="color:#f92672">&lt;</span> num_submaps ) <span style="color:#f92672">++</span>submap_idx;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (submap_idx <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>      submap_offset <span style="color:#f92672">=</span> tid <span style="color:#f92672">-</span> prefix_sum[submap_idx <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>];
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span> <span style="color:#f92672">&amp;</span>current_slot  <span style="color:#f92672">=</span> submap_views[submap_idx].get_slots()[submap_offset];
</span></span><span style="display:flex;"><span>    Key <span style="color:#66d9ef">const</span> existing_key    <span style="color:#f92672">=</span> current_slot.first.load(cuda<span style="color:#f92672">::</span>std<span style="color:#f92672">::</span>memory_order_relaxed);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> <span style="color:#66d9ef">const</span> is_filled <span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>      not(cuco<span style="color:#f92672">::</span>detail<span style="color:#f92672">::</span>bitwise_compare(existing_key, empty_key_sentinel) or
</span></span><span style="display:flex;"><span>          cuco<span style="color:#f92672">::</span>detail<span style="color:#f92672">::</span>bitwise_compare(existing_key, erased_key_sentinel));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> tile_mask <span style="color:#f92672">=</span> tile.ballot(is_filled);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> tile_rank <span style="color:#f92672">=</span> tile.thread_rank();
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> tile_vote <span style="color:#f92672">=</span> __popc(tile_mask);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> tile_prefix <span style="color:#f92672">=</span> __popc(tile_mask <span style="color:#f92672">&amp;</span> ((<span style="color:#ae81ff">1u</span> <span style="color:#f92672">&lt;&lt;</span> tile_rank) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>));
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> <span style="color:#66d9ef">int</span> tile_base <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (tile_rank <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&amp;&amp;</span> tile_mask) {
</span></span><span style="display:flex;"><span>      tile_base <span style="color:#f92672">=</span> atomicAdd_block(<span style="color:#f92672">&amp;</span>block_count, tile_vote);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    tile_base <span style="color:#f92672">=</span> tile.shfl(tile_base, <span style="color:#ae81ff">0</span>);
</span></span><span style="display:flex;"><span>    block.sync();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (block.thread_rank() <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) {
</span></span><span style="display:flex;"><span>      block_base <span style="color:#f92672">=</span> atomicAdd(d_num_out, block_count);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    block.sync();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (is_filled) {
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">auto</span> value <span style="color:#f92672">=</span> current_slot.second.load(cuda<span style="color:#f92672">::</span>std<span style="color:#f92672">::</span>memory_order_relaxed);
</span></span><span style="display:flex;"><span>      keys_out[block_base <span style="color:#f92672">+</span> tile_base <span style="color:#f92672">+</span> tile_prefix]    <span style="color:#f92672">=</span> existing_key;
</span></span><span style="display:flex;"><span>      values_out[block_base <span style="color:#f92672">+</span> tile_base <span style="color:#f92672">+</span> tile_prefix]  <span style="color:#f92672">=</span> value;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>实测2/3速度差不多，在插入1亿数据后（实际总cap达到2亿），<code>&lt;key, value&gt;</code>都是<code>cuda::atomic&lt;int64_t&gt;</code>的情况下，retrieve_all cost 3ms左右；</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://caaatch22.github.io/posts/tinyml-quantization/">
    <span class="title">Next »</span>
    <br>
    <span>TinyMl —— 模型量化(quantization)</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://caaatch22.github.io/">Mingjie&#39;s Home</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
